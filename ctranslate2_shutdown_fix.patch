diff --git a/include/ctranslate2/replica_pool.h b/include/ctranslate2/replica_pool.h
index 8c8e15d8..2ec1fc47 100644
--- a/include/ctranslate2/replica_pool.h
+++ b/include/ctranslate2/replica_pool.h
@@ -309,6 +309,7 @@ namespace ctranslate2 {
       , _device_index(model->device_index())
       , _num_threads(num_threads)
       , _allocator(nullptr)
+      , _shutting_down(false)
     {
       set_model(model);
     }
@@ -347,12 +348,34 @@ namespace ctranslate2 {
     }
 
     void idle() override {
+      // Use atomic load with acquire semantics to ensure we see the shutdown flag
+      // even if it was just set by another thread. This prevents the race condition
+      // where idle() starts executing just before finalize() sets _shutting_down.
+      if (_shutting_down.load(std::memory_order_acquire))
+        return;
+
       // When no new jobs are immediately available, we synchronize the CUDA stream
       // so that the CudaAsyncAllocator can release some memory.
+      // Note: This can block if there's pending GPU work, but that's expected
+      // during normal operation. The shutdown flag check above prevents this
+      // from blocking during destructor cleanup.
       synchronize_stream(_device);
     }
 
     void finalize() override {
+      // Mark as shutting down with release semantics to ensure visibility
+      // to idle() checks. This must happen first to prevent idle() from
+      // calling synchronize_stream() after we start cleanup.
+      _shutting_down.store(true, std::memory_order_release);
+
+      // Don't call synchronize_stream during shutdown - it can block indefinitely
+      // on Windows when CUDA resources are being torn down. The replica reset
+      // will handle cleanup through CUDA's own destruction sequence.
+      //
+      // Note: We intentionally skip synchronization here. The CUDA driver will
+      // handle proper cleanup when the context is destroyed. Trying to synchronize
+      // at this point can deadlock if there are pending operations or if the
+      // CUDA context is in an inconsistent state.
       _replica.reset();
     }
 
@@ -362,6 +385,7 @@ namespace ctranslate2 {
     const size_t _num_threads;
     Allocator* _allocator;
     std::unique_ptr<Replica> _replica;
+    std::atomic<bool> _shutting_down;
   };
 
 }
diff --git a/include/ctranslate2/thread_pool.h b/include/ctranslate2/thread_pool.h
index 826b7e57..b05bf15b 100644
--- a/include/ctranslate2/thread_pool.h
+++ b/include/ctranslate2/thread_pool.h
@@ -59,7 +59,11 @@ namespace ctranslate2 {
     virtual ~Worker() = default;
 
     void start(JobQueue& job_queue, int thread_affinity = -1);
-    void join();
+
+    // Join the worker thread. If timeout_ms > 0 and the thread doesn't finish
+    // in time, the thread is detached to prevent indefinite blocking.
+    // This is critical on Windows where CUDA cleanup can hang during shutdown.
+    void join(int timeout_ms = 5000);
 
   protected:
     // Called before the work loop.
diff --git a/src/cuda/primitives.cu b/src/cuda/primitives.cu
index 9915bb12..5de56514 100644
--- a/src/cuda/primitives.cu
+++ b/src/cuda/primitives.cu
@@ -457,15 +457,19 @@ namespace ctranslate2 {
                                       float* c, dim_t ldc,
                                       const float*) {
     // cuBLAS assumes column-major storage, so swap a and b accordingly.
-    CUBLAS_CHECK(cublasSgemm(cuda::get_cublas_handle(),
-                             transpose_b ? CUBLAS_OP_T : CUBLAS_OP_N,
-                             transpose_a ? CUBLAS_OP_T : CUBLAS_OP_N,
-                             n, m, k,
-                             &alpha,
-                             b, ldb,
-                             a, lda,
-                             &beta,
-                             c, ldc));
+    // Using cublasGemmEx with CUBLAS_GEMM_DEFAULT_TENSOR_OP enables TF32 on Ampere+ GPUs
+    // for ~3x speedup with minimal accuracy loss.
+    CUBLAS_CHECK(cublasGemmEx(cuda::get_cublas_handle(),
+                              transpose_b ? CUBLAS_OP_T : CUBLAS_OP_N,
+                              transpose_a ? CUBLAS_OP_T : CUBLAS_OP_N,
+                              n, m, k,
+                              &alpha,
+                              b, CUDA_R_32F, ldb,
+                              a, CUDA_R_32F, lda,
+                              &beta,
+                              c, CUDA_R_32F, ldc,
+                              CUDA_R_32F,
+                              CUBLAS_GEMM_DEFAULT_TENSOR_OP));
   }
 
   template<>
@@ -570,16 +574,20 @@ namespace ctranslate2 {
                                                     float* c, dim_t ldc, dim_t stridec,
                                                     dim_t batch_size) {
     // cuBLAS assumes column-major storage, so swap a and b accordingly.
-    CUBLAS_CHECK(cublasSgemmStridedBatched(cuda::get_cublas_handle(),
-                                           transpose_b ? CUBLAS_OP_T : CUBLAS_OP_N,
-                                           transpose_a ? CUBLAS_OP_T : CUBLAS_OP_N,
-                                           n, m, k,
-                                           &alpha,
-                                           b, ldb, strideb,
-                                           a, lda, stridea,
-                                           &beta,
-                                           c, ldc, stridec,
-                                           batch_size));
+    // Using cublasGemmStridedBatchedEx with CUBLAS_GEMM_DEFAULT_TENSOR_OP enables TF32
+    // on Ampere+ GPUs for ~3x speedup with minimal accuracy loss.
+    CUBLAS_CHECK(cublasGemmStridedBatchedEx(cuda::get_cublas_handle(),
+                                            transpose_b ? CUBLAS_OP_T : CUBLAS_OP_N,
+                                            transpose_a ? CUBLAS_OP_T : CUBLAS_OP_N,
+                                            n, m, k,
+                                            &alpha,
+                                            b, CUDA_R_32F, ldb, strideb,
+                                            a, CUDA_R_32F, lda, stridea,
+                                            &beta,
+                                            c, CUDA_R_32F, ldc, stridec,
+                                            batch_size,
+                                            CUDA_R_32F,
+                                            CUBLAS_GEMM_DEFAULT_TENSOR_OP));
   }
 
   template<>
diff --git a/src/thread_pool.cc b/src/thread_pool.cc
index d0aad775..56b6fec9 100644
--- a/src/thread_pool.cc
+++ b/src/thread_pool.cc
@@ -2,6 +2,10 @@
 
 #include "ctranslate2/utils.h"
 
+#include <chrono>
+#include <future>
+#include <iostream>
+
 namespace ctranslate2 {
 
   Job::~Job() {
@@ -46,9 +50,18 @@ namespace ctranslate2 {
   std::unique_ptr<Job> JobQueue::get(const std::function<void()>& before_wait) {
     std::unique_lock<std::mutex> lock(_mutex);
 
-    if (!can_get_job()) {
-      if (before_wait)
+    while (!can_get_job()) {
+      // Only call before_wait() if we're not shutting down.
+      // This is critical because before_wait() may call cudaStreamSynchronize() which
+      // can block indefinitely if there's pending work, preventing clean shutdown.
+      if (before_wait && !_request_end) {
+        // Release lock before calling before_wait() to avoid blocking while holding mutex.
+        lock.unlock();
         before_wait();
+        lock.lock();
+        // Re-check condition after re-acquiring lock - use continue to re-evaluate loop
+        continue;
+      }
       _can_get_job.wait(lock, [this]{ return can_get_job(); });
     }
 
@@ -102,8 +115,46 @@ namespace ctranslate2 {
       set_thread_affinity(_thread, thread_affinity);
   }
 
-  void Worker::join() {
-    _thread.join();
+  void Worker::join(int timeout_ms) {
+    if (!_thread.joinable()) {
+      return;
+    }
+
+    if (timeout_ms <= 0) {
+      // No timeout - block indefinitely (original behavior)
+      _thread.join();
+      return;
+    }
+
+    // Use a future to implement timed join
+    // We spawn a helper thread that waits for the worker to finish
+    std::promise<void> join_promise;
+    std::future<void> join_future = join_promise.get_future();
+
+    std::thread join_helper([this, &join_promise]() {
+      if (_thread.joinable()) {
+        _thread.join();
+      }
+      join_promise.set_value();
+    });
+
+    // Wait with timeout
+    auto status = join_future.wait_for(std::chrono::milliseconds(timeout_ms));
+
+    if (status == std::future_status::ready) {
+      // Thread finished in time
+      join_helper.join();
+    } else {
+      // Timeout - detach both threads to prevent blocking
+      // This is safe because the worker thread will eventually finish
+      // and we're in destructor context anyway
+      std::cerr << "[CTranslate2] Warning: Worker thread join timed out after "
+                << timeout_ms << "ms, detaching to prevent hang\n";
+      join_helper.detach();
+      if (_thread.joinable()) {
+        _thread.detach();
+      }
+    }
   }
 
   void Worker::run(JobQueue& job_queue) {
